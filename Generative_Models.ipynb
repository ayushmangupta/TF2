{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative Models",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmangupta/TF2/blob/master/Generative_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjI-G6gq5XRX",
        "colab_type": "text"
      },
      "source": [
        "- Hierarchical Bayesian models \n",
        "- Multivariate kernel methods \n",
        "- Discriminative machine learning \n",
        "- Clustering algorithms\n",
        "- Dimensionality reduction \n",
        "\n",
        "# Differentiable Inference and Generative Models\n",
        "\n",
        "**What are generative models?**\n",
        "\n",
        "Generative modeling loosely refers to building a model of data, for instance p(image), that we can sample from. This is in contrast to discriminative modeling, such as regression or classification, which tries to estimate conditional distributions such as p(class | image).\n",
        "\n",
        "**Why generative models?**\n",
        "\n",
        "Even when we're only interested in making predictions, there are practical reasons to build generative models:\n",
        "\n",
        "Data efficiency and semi-supervised learning - Generative models can reduce the amount of data required. As a simple example, building an image classifier $p(class | image) $ requires estimating a very high-dimenisonal function, possibly requiring a lot of data, or clever assumptions. In contrast, we could model the data as being generated from some low-dimensional or sparse latent variables $z$, as in $ p(image)=âˆ«p(image|z)p(z)dz$. Then, to do classification, we only need to learn $p( class | z)$ , which will usually be a much simpler function. This approach also lets us take advantage of unlabeled data - also known as semi-supervised learning.\n",
        "Model checking by sampling - Understanding complex regression and classification models is hard - it's often not clear what these models have learned from the data and what they missed. There is a simple way to sanity-check and inspect generative models - simply sample from them, and compare the sampled data to the real data to see if anything is missing.\n",
        "Understanding - Generative models usually assume that each datapoint is generated from a (usually low-dimensional) latent variable. These latent variables are often interpretable, and sometimes can tell us about the hidden causes of a phenomenon. These latent variables can also sometimes let us do interesting things such as interpolating between examples\n",
        "\n",
        "### Types of Generative Models\n",
        " \n",
        " - VAE\n",
        " - Auto Regressive Models\n",
        " - GAN's\n",
        "\n",
        "___________________\n",
        "- Conditional probabilistic models\n",
        "-  Latent-variable probabilistic models\n",
        "-  GANs\n",
        "-  Invertible models\n",
        "\n",
        "### Advantages of latent variable models\n",
        "- Model checking by sampling\n",
        "- Natural way to specify models\n",
        "- Compact representations\n",
        "- Semi-Supervised learning\n",
        "- Understanding factors of variation in data\n",
        "\n",
        "### Advantages of probabilistic latent-variable models\n",
        "- Data-efficient learning - automatic regularization, can take advantage of more information\n",
        "- Compose models - e.g. incorporate data corruption model. Different from composing\n",
        "feedforward computations\n",
        "- Handle missing data (without the standard hack of just guessing the missing values using\n",
        "averages).\n",
        "- Predictive uncertainty - necessary for decision-making\n",
        "- conditional predictions (e.g. if brexit happens, the value of the pound will fall)\n",
        "- Active learning - what data would be expected to increase our confidence about a\n",
        "prediction\n",
        "\n",
        "- **Con**\n",
        "\n",
        "  -  intractable integral over latent variables \n",
        "\n",
        "\n",
        "\n",
        "Lectures\n",
        "\n",
        "- http://www.cs.toronto.edu/~duvenaud/courses/csc2541/\n",
        "- https://www.adhiraiyan.org/deeplearning/03.00-Probability-and-Information-Theory\n",
        "- Density estimation using Real NVP (youtube explanation and paper)\n",
        "- Deep Learning with TF2 Book tf2 version [Link](https://github.com/adhiraiyan/DeepLearningWithTF2.0/blob/44deed6175d321c807cfc4812df6d0fed633f8bf/notebooks/04.00-Numerical-Computation.ipynb)\n",
        "- Synthesising Image [Paper](https://arxiv.org/pdf/1605.09304.pdf)\n",
        "- Unsupervised Representation Learning with Deep Convolutional Generative\n",
        "Adversarial Networks\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVU8I8Tq40cO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}