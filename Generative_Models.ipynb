{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative Models",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmangupta/TF2/blob/master/Generative_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjI-G6gq5XRX",
        "colab_type": "text"
      },
      "source": [
        "- Hierarchical Bayesian models \n",
        "- Multivariate kernel methods \n",
        "- Discriminative machine learning \n",
        "- Clustering algorithms\n",
        "- Dimensionality reduction \n",
        "\n",
        "# Differentiable Inference and Generative Models\n",
        "\n",
        "**What are generative models?**\n",
        "\n",
        "Generative modeling loosely refers to building a model of data, for instance p(image), that we can sample from. This is in contrast to discriminative modeling, such as regression or classification, which tries to estimate conditional distributions such as p(class | image).\n",
        "\n",
        "**Why generative models?**\n",
        "\n",
        "Even when we're only interested in making predictions, there are practical reasons to build generative models:\n",
        "\n",
        "Data efficiency and semi-supervised learning - Generative models can reduce the amount of data required. As a simple example, building an image classifier $p(class | image) $ requires estimating a very high-dimenisonal function, possibly requiring a lot of data, or clever assumptions. In contrast, we could model the data as being generated from some low-dimensional or sparse latent variables $z$, as in $ p(image)=∫p(image|z)p(z)dz$. Then, to do classification, we only need to learn $p( class | z)$ , which will usually be a much simpler function. This approach also lets us take advantage of unlabeled data - also known as semi-supervised learning.\n",
        "Model checking by sampling - Understanding complex regression and classification models is hard - it's often not clear what these models have learned from the data and what they missed. There is a simple way to sanity-check and inspect generative models - simply sample from them, and compare the sampled data to the real data to see if anything is missing.\n",
        "Understanding - Generative models usually assume that each datapoint is generated from a (usually low-dimensional) latent variable. These latent variables are often interpretable, and sometimes can tell us about the hidden causes of a phenomenon. These latent variables can also sometimes let us do interesting things such as interpolating between examples\n",
        "\n",
        "### Types of Generative Models\n",
        " \n",
        " - VAE\n",
        " - Auto Regressive Models\n",
        " - GAN's\n",
        "\n",
        "\n",
        "Lectures\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIB1UurUHkSP",
        "colab_type": "text"
      },
      "source": [
        "# VAE\n",
        "###\n",
        "- VAEs are trained by maximizing the variational lowerbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzXTo2C1xfnC",
        "colab_type": "text"
      },
      "source": [
        "# GAN with Objective Function\n",
        "\n",
        "### GAN:\n",
        "-  GANs do not require any approximation and can be trainedend-to-end through the differentiable network\n",
        "-   The basic idea of GANs is tosimultaneously  train  a  discriminator  and  a  generator:  the  discriminator  aimsto distinguish between real samples and generated samples;  while the genera-tor tries to generate fake samples as real as possible, making the discriminatorbelieve that the fake samples are from real data.\n",
        "\n",
        "### Application\n",
        "- Image generation \n",
        "  - Infogan\n",
        "- Image super-resolution \n",
        "  - Image Super-Resolution Using a Generative Adversarial Network\n",
        "- Text to image synthesis \n",
        "  - Gen-erative  adversarial  text-to-image  synthesis\n",
        "- Image to image translation\n",
        "  - Image-to-image translationwith conditional adversarial networks\n",
        "  \n",
        "  \n",
        "\n",
        "| GAN        | Objective Function           | \n",
        "| ------------- |:-------------:| \n",
        "|        GAN(Orignal)| JSD Divergence           | \n",
        "|        WGAN | EM Distance           | \n",
        "|        Improved WGAN| No weight Clipping       | \n",
        "|        LSGAN | L2 Loss Objective         | \n",
        "|        RWGAN| Relaxed WGAN     | \n",
        "|        McGAN| Mean Covariance Minimization      | \n",
        "|       GMMN | Maximum Mean Discrepency | \n",
        "|       MMD GAN | Adversial Kernel To GMMN |\n",
        "|       Cramer GAN | Gramer Distance |\n",
        "|       Fisher GAN   | Chi-Square Objective |\n",
        "|       EBGAN |  AutoEncoder instead of Discriminator|\n",
        "|       BEGAN | WGAN and EBGAN Merged Objective |\n",
        "|       MAGAN| Dynamic Margin on Hinge Loss for EBGAN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Jensen Shannon Divergence :\n",
        "P and Q are probability measures, then the Jensen-Shannon Divergence is: \n",
        "$$ (P,Q) = KL(P||R) + KL(Q||R)  $$  where $R = \\frac{1}{2} (P+Q)$\n",
        "- R is the mid-point measure and KL(⋅∣∣⋅) is the Kullback-Leibler divergence.\n",
        "\n",
        "\n",
        "#### WGAN:\n",
        "WGAN doesn’t use the JSD to measure divergence, instead it uses something called the Earth-Mover (EM) distance (AKA Wasserstein distance). EM distance is defined as\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### LSGAN(Least Square GAN) [Paper](https://arxiv.org/pdf/1611.04076.pdf)\n",
        "-  The loss for real samples should be lower than the loss for fake samples. This allows the LSGAN to put a high focus on fake samples that have a really high margin.\n",
        "- They introduce regularization in the form of weight decay, encouraging the weights of their function to lie within a bounded area that guarantee the theoretical needs.\n",
        "\n",
        "- **Imp**The reason for this has to do with the fact that a log loss will basically only care about whether or not a sample is labeled correctly or not. It will not heavily penalize based on the distance of said sample from correct classification. If a label is correct, it doesn’t worry about it further. In contrast, L2 loss does care about distance.\n",
        "Data far away from where it should be will be penalized proportionally. What LSGAN argues is that this produces more informative gradients.\n",
        " \n",
        " -    Loss function instead of a critic\n",
        " -    Weight decay regularization to bound loss function\n",
        " -    L2 loss instead of log loss for proportional penalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV3wWXECI9iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N59bRc9Hxd0B",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "- http://www.cs.toronto.edu/~duvenaud/courses/csc2541/\n",
        "- https://www.adhiraiyan.org/deeplearning/03.00-Probability-and-Information-Theory\n",
        "- Density estimation using Real NVP (youtube explanation and paper)\n",
        "- Deep Learning with TF2 Book tf2 version [Link](https://github.com/adhiraiyan/DeepLearningWithTF2.0/blob/44deed6175d321c807cfc4812df6d0fed633f8bf/notebooks/04.00-Numerical-Computation.ipynb)\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVU8I8Tq40cO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}