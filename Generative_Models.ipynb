{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative Models",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmangupta/TF2/blob/master/Generative_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LglXgoOgK0o",
        "colab_type": "text"
      },
      "source": [
        "# Terminology \n",
        "\n",
        "**PDF :** is a non-negetive function which integrates to one. The likelihood is defined as the joint density of the observed data as a function of the parameter. \n",
        "\n",
        "**Likelihood Function:** likelihood function is a function of the parameter only, with the data held as a fixed constant.\n",
        "- The likelihood function is a function of the unknown parameter θ (conditioned on the data). As such, it does typically not have area 1 (i.e. the integral over all possible values of θ is not 1) and is therefore by definition not a pdf\n",
        "\n",
        "**Wasserstein distances**  It commonly replaces the Kullback-Leibler divergence (also often dubbed cross-entropy loss in the Deep Learning context). In contrast to the latter, Wasserstein distances not only consider the values probability distribution or density at any given point, but also incorporating spatial information in terms of the underlying metric regarding these differences. Intuitively, it yields a smaller distance if probability mass moved to a nearby point or region and a larger distance if probability mass moved far away.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjI-G6gq5XRX",
        "colab_type": "text"
      },
      "source": [
        "- Hierarchical Bayesian models \n",
        "- Multivariate kernel methods \n",
        "- Discriminative machine learning \n",
        "- Clustering algorithms\n",
        "- Dimensionality reduction \n",
        "\n",
        "\n",
        "\n",
        "# Differentiable Inference and Generative Models\n",
        "\n",
        "**What are generative models?**\n",
        "\n",
        "Generative modeling loosely refers to building a model of data, for instance p(image), that we can sample from. This is in contrast to discriminative modeling, such as regression or classification, which tries to estimate conditional distributions such as p(class | image).\n",
        "\n",
        "**Why generative models?**\n",
        "\n",
        "Even when we're only interested in making predictions, there are practical reasons to build generative models:\n",
        "\n",
        "Data efficiency and semi-supervised learning - Generative models can reduce the amount of data required. As a simple example, building an image classifier $p(class | image) $ requires estimating a very high-dimenisonal function, possibly requiring a lot of data, or clever assumptions. In contrast, we could model the data as being generated from some low-dimensional or sparse latent variables $z$, as in $ p(image)=∫p(image|z)p(z)dz$. Then, to do classification, we only need to learn $p( class | z)$ , which will usually be a much simpler function. This approach also lets us take advantage of unlabeled data - also known as semi-supervised learning.\n",
        "Model checking by sampling - Understanding complex regression and classification models is hard - it's often not clear what these models have learned from the data and what they missed. There is a simple way to sanity-check and inspect generative models - simply sample from them, and compare the sampled data to the real data to see if anything is missing.\n",
        "Understanding - Generative models usually assume that each datapoint is generated from a (usually low-dimensional) latent variable. These latent variables are often interpretable, and sometimes can tell us about the hidden causes of a phenomenon. These latent variables can also sometimes let us do interesting things such as interpolating between examples\n",
        "\n",
        "### Types of Generative Models\n",
        " \n",
        " - VAE\n",
        " - Auto Regressive Models\n",
        " - GAN's\n",
        "\n",
        "\n",
        "Lectures\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIB1UurUHkSP",
        "colab_type": "text"
      },
      "source": [
        "# VAE\n",
        "###\n",
        "- VAEs are trained by maximizing the variational lowerbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwKdFuml1A9o",
        "colab_type": "text"
      },
      "source": [
        "# GAN with Objective Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0qm_vck1Hys",
        "colab_type": "text"
      },
      "source": [
        "### GAN:\n",
        "-  GANs do not require any approximation and can be trainedend-to-end through the differentiable network\n",
        "-   The basic idea of GANs is tosimultaneously  train  a  discriminator  and  a  generator:  the  discriminator  aimsto distinguish between real samples and generated samples;  while the genera-tor tries to generate fake samples as real as possible, making the discriminatorbelieve that the fake samples are from real data.\n",
        "\n",
        "### Application\n",
        "- Image generation \n",
        "  - Infogan\n",
        "- Image super-resolution \n",
        "  - Image Super-Resolution Using a Generative Adversarial Network\n",
        "- Text to image synthesis \n",
        "  - Gen-erative  adversarial  text-to-image  synthesis\n",
        "- Image to image translation\n",
        "  - Image-to-image translationwith conditional adversarial networks\n",
        "  \n",
        "  \n",
        "\n",
        "| GAN        | Objective Function           | \n",
        "| ------------- |:-------------:| \n",
        "|        GAN(Orignal)| JSD Divergence           | \n",
        "|        WGAN | EM Distance           | \n",
        "|        Improved WGAN| No weight Clipping       | \n",
        "|        LSGAN | L2 Loss Objective         | \n",
        "|        RWGAN| Relaxed WGAN     | \n",
        "|        McGAN| Mean Covariance Minimization      | \n",
        "|       GMMN | Maximum Mean Discrepency | \n",
        "|       MMD GAN | Adversial Kernel To GMMN |\n",
        "|       Cramer GAN | Gramer Distance |\n",
        "|       Fisher GAN   | Chi-Square Objective |\n",
        "|       EBGAN |  AutoEncoder instead of Discriminator|\n",
        "|       BEGAN | WGAN and EBGAN Merged Objective |\n",
        "|       MAGAN| Dynamic Margin on Hinge Loss for EBGAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqk8FY3a1NRa",
        "colab_type": "text"
      },
      "source": [
        "# GAN's Variations and their loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzXTo2C1xfnC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Jensen Shannon Divergence :\n",
        "P and Q are probability measures, then the Jensen-Shannon Divergence is: \n",
        "$$ (P,Q) = KL(P||R) + KL(Q||R)  $$  where $R = \\frac{1}{2} (P+Q)$\n",
        "- R is the mid-point measure and KL(⋅∣∣⋅) is the Kullback-Leibler divergence.\n",
        "\n",
        "\n",
        "### WGAN:\n",
        "WGAN doesn’t use the JSD to measure divergence, instead it uses something called the Earth-Mover (EM) distance (AKA Wasserstein distance). EM distance is defined as\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### LSGAN(Least Square GAN) [Paper](https://arxiv.org/pdf/1611.04076.pdf)\n",
        "-  The loss for real samples should be lower than the loss for fake samples. This allows the LSGAN to put a high focus on fake samples that have a really high margin.\n",
        "- They introduce regularization in the form of weight decay, encouraging the weights of their function to lie within a bounded area that guarantee the theoretical needs.\n",
        "\n",
        "- **Imp**The reason for this has to do with the fact that a log loss will basically only care about whether or not a sample is labeled correctly or not. It will not heavily penalize based on the distance of said sample from correct classification. If a label is correct, it doesn’t worry about it further. In contrast, L2 loss does care about distance.\n",
        "Data far away from where it should be will be penalized proportionally. What LSGAN argues is that this produces more informative gradients.\n",
        " \n",
        " -    Loss function instead of a critic\n",
        " -    Weight decay regularization to bound loss function\n",
        " -    L2 loss instead of log loss for proportional penalization\n",
        "\n",
        "\n",
        "### Cramer GAN\n",
        "Cramer GAN starts by outlining an issue with the popular WGAN. It claims that there are three properties that a probability divergence should satisfy:\n",
        "\n",
        "    Sum invariance\n",
        "    Scale sensitivity\n",
        "    Unbiased sample gradients\n",
        "\n",
        "Of these properties, they argue that the Wasserstein distance lacks the final property, unlike KLD or JSD which both have it. They demonstrate that this is actually an issue in practice, and propose a new distance: the Cramer distance.\n",
        "The Cramer Distance\n",
        "\n",
        "Now if we look at the Cramer distance, we can actually see it looks somewhat similar to the EM distance. However, due to its mathematical differences, it actually doesn’t suffer from the biased sample gradients that EM distance will. This is proven in the paper, if you really wish to dig into the mathematics of it.\n",
        " \n",
        " \n",
        " ### EBGAN \n",
        " Setting Up\n",
        "   -  1. Train an autoencoder on the original data\n",
        "    2. Now run generated images through this autoencoder\n",
        "    3. Poorly generated images will have awful reconstruction loss, and thus this now becomes a good measure\n",
        "    \n",
        " Summary\n",
        "    - Autoencoder as the discriminator\n",
        "    - Reconstruction loss used as cost, setup similar to original GAN cost\n",
        "    - Fast, stable, and robust\n",
        "    \n",
        "    \n",
        "### Boundary Equilibrium GAN\n",
        "\n",
        "Boundary Equilibrium GAN (BEGAN) is an iteration on EBGAN. It instead uses the autoencoder reconstruction loss in a way that is similar to WGAN’s loss function.\n",
        "\n",
        "In order to do this, a parameter needs to be introduced to balance the training of the discriminator and generator. This parameter is weighted as a running mean over the samples, dancing at the boundary between improving the two halves (thus where it gets its name: “boundary equilibrium”).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV3wWXECI9iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N59bRc9Hxd0B",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "- http://www.cs.toronto.edu/~duvenaud/courses/csc2541/\n",
        "- https://www.adhiraiyan.org/deeplearning/03.00-Probability-and-Information-Theory\n",
        "- Density estimation using Real NVP (youtube explanation and paper)\n",
        "- Deep Learning with TF2 Book tf2 version [Link](https://github.com/adhiraiyan/DeepLearningWithTF2.0/blob/44deed6175d321c807cfc4812df6d0fed633f8bf/notebooks/04.00-Numerical-Computation.ipynb)\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVU8I8Tq40cO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}